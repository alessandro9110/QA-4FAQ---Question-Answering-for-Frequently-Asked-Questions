{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Bi-LSTM - Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import certifi\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import keras\n",
    "import h5py\n",
    "import pydot\n",
    "import os\n",
    "import sklearn\n",
    "import pickle\n",
    "\n",
    "from keras.layers.merge import Add\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Embedding, Merge, LSTM, Bidirectional, GlobalMaxPooling1D, Dot, Lambda, Dropout\n",
    "from keras.layers import Average, dot,Permute, Lambda,Layer, add, Concatenate, Dense, LSTM, Input, concatenate, merge, Add, Reshape, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.models import Sequential\n",
    "from numpy import newaxis\n",
    "from keras.models import model_from_json\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from collections import defaultdict\n",
    "from scipy.stats.stats import pearsonr\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "K.os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "K.os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD WORD2VEC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD GENSIM MODEL WITH WORD2VEC\n",
    "model_W2C = gensim.models.Word2Vec.load('model/mymodelFull')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA TRAINING MANIPULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TOKENIZER FILES\n",
    "def loadSentences(filename):\n",
    "    array = []\n",
    "    with open(filename) as f:\n",
    "        ar = []\n",
    "        for line in f:\n",
    "            if (str(line) != \"[\\n\") & (str(line) != \"]\\n\"):\n",
    "                ar.append(re.sub('\\n$', '', line))\n",
    "                #print (line)\n",
    "            if (str(line) == \"]\\n\"):\n",
    "                array.append(ar)\n",
    "                ar = []\n",
    "            \n",
    "    f.close()\n",
    "    return array\n",
    "\n",
    "#ANSWER TRAINING\n",
    "original_QA_train = loadSentences(\"tokenizer/tokenizerQuestionAnswer.txt\")\n",
    "QA_train = loadSentences(\"tokenizer/tokenizerQuestionAnswer.txt\")\n",
    "\n",
    "#QUESTION  TRAINING\n",
    "original_Q_train = loadSentences(\"tokenizer/tokenizerQuestion.txt\")\n",
    "Q_train = loadSentences(\"tokenizer/tokenizerQuestion.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(QA_train[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET MANIPULATION FOR TRAINING\n",
    "#CREATE 2 TRAIN WITH CORRECT QUESTION-ANSWER AND INCORRECT QUESTION-ANSWER\n",
    "new = []\n",
    "#A_train raddoppiamo il train aggiungendo risposte casuali\n",
    "for i in range(0,len(original_QA_train)):\n",
    "    new.append(random.choice(QA_train))\n",
    "QA_train.extend(new)\n",
    "print(len(QA_train))\n",
    "\n",
    "new = []\n",
    "#QA_train raddoppiamo il train aggiungendo risposte casuali\n",
    "for i in range(0,len(original_Q_train)):\n",
    "    new.append(random.choice(Q_train))\n",
    "Q_train.extend(new)\n",
    "print(len(Q_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train LABEL CLASS FOR EVALUATION TRAINING\n",
    "Y_train = []\n",
    "for i in range(0, (int(len(QA_train)/2))):\n",
    "    a=[]\n",
    "    a.append(1)\n",
    "    Y_train.append(a)\n",
    "for i in range(int(len(QA_train)/2+1), len(QA_train) + 1):\n",
    "    a=[]\n",
    "    a.append(0)\n",
    "    Y_train.append(a)\n",
    "Y_train = np.array(Y_train)\n",
    "print(len(Q_train),len(QA_train),len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBSTITUTE WORD WITH INDEX\n",
    "def replace_matched_items(array, dictionary):\n",
    "       for lst in array:\n",
    "              for ind, item in enumerate(lst):\n",
    "                          lst[ind] = dictionary.get(item, item)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN ZEROS\n",
    "def cleaningZero(array):\n",
    "       for elem in array:\n",
    "            while 0 in elem: \n",
    "                elem.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vocab - uso lo stesso vocab\n",
    "vocab = dict([(k, v.index) for k, v in model_W2C.wv.vocab.items()])\n",
    "\n",
    "#REPLACE A_TRAIN AND Q_TRAIN WITH VOCAB INDEX\n",
    "replace_matched_items(Q_train,vocab)\n",
    "cleaningZero(Q_train)\n",
    "replace_matched_items(QA_train,vocab)\n",
    "cleaningZero(QA_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAD DOCUMENTS FOR QUESTION AND ANSWER\n",
    "max_len = max(len(max(QA_train,key=len)), len(max(Q_train,key=len)))\n",
    "Q_train_padded = keras.preprocessing.sequence.pad_sequences(Q_train, padding='post', maxlen = max_len)\n",
    "QA_train_padded = keras.preprocessing.sequence.pad_sequences(QA_train, padding='post', maxlen = max_len)\n",
    "#print(Q_train_padded.shape, A_train_padded.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS MODEL (INPUTS: Q - QA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTON FOR MATRIX MULTIPLICATION\n",
    "class OuterProduct(Layer):\n",
    "    def __init__(self,name=\"OuterProduct\",**kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.length = input_shape[0][1]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        a, b = x\n",
    "#        print(a.shape,b.shape,a[:, newaxis, :].shape,b[:, :, newaxis].shape)\n",
    "        outerProduct = a[:, newaxis, :] * b[:, :, newaxis]\n",
    " #       print(outerProduct.shape)\n",
    "        outerProduct = K.sum(outerProduct,axis=-1)\n",
    "  #      print(outerProduct.shape)\n",
    "\n",
    "        return outerProduct\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], input_shape[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "#Save the model after every epoch. https://keras.io/callbacks/#modelcheckpoint\n",
    "checkpoint = ModelCheckpoint(\"keras_models/Q_QA_Avg.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#Stop training when a monitored quantity has stopped improving. https://keras.io/callbacks/#earlystopping\n",
    "stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "csv_logger = CSVLogger('history/AVG_Q-QA/log.csv', append=True, separator=';')\n",
    "\n",
    "callbacks_list = [checkpoint,stop,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LAYER EMBEDDING-BiLSTM FOR ANSWER\n",
    "QA_input = Input(batch_shape=(None,QA_train_padded.shape[1],), name='QA_input')\n",
    "QA = Embedding(len(vocab),output_dim=200,name=\"Embedding_A\")(QA_input)\n",
    "QA_LSTM = Bidirectional(LSTM(100, dropout=0.6, recurrent_dropout=0.5,return_sequences=True),name=\"LSTM_A\")(QA)\n",
    "\n",
    "\n",
    "#LAYER EMBEDDING-BiLSTM FOR QUESTIONS\n",
    "Q_input = Input(batch_shape=(None,Q_train_padded.shape[1],),  name='Q_input')\n",
    "Q = Embedding(len(vocab),output_dim=200,name=\"Embedding_Q\")(Q_input)\n",
    "Q_LSTM = Bidirectional(LSTM(100, dropout=0.6, recurrent_dropout=0.5,return_sequences=True),name=\"LSTM_QA\")(Q)\n",
    "\n",
    "#AVERAGE EMBEDDING\n",
    "M1 = Average()([Q,QA])\n",
    "\n",
    "#AVERAGE BI-LSTM\n",
    "M2 = Average()([Q_LSTM, QA_LSTM])\n",
    "\n",
    "M12 = Concatenate(axis=1,name=\"M1avgM2\")([M1, M2])\n",
    "\n",
    "#MLP\n",
    "\n",
    "dense = Dense(200, activation='relu')(Reshape((-1,))(M12))\n",
    "dense = Dropout(.6)(dense)\n",
    "dense= Dense(100, activation='relu')(dense)\n",
    "dense = Dropout(.6)(dense)\n",
    "\n",
    "output = Dense(1,activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=[Q_input,QA_input], outputs=output)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "#model.summary(line_length=200)\n",
    "print(\"Model Ready...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT PNG KERAS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINT MODEL LAYERS ON FILE.PNG\n",
    "plot_model(model, to_file='image/model_Q_QA_Avg.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL Q - QA\n",
    "Stratified K-Fold with 10 splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "def train_and_evaluate__model(m, q1, a2,y3,q4,a5,y6,I):\n",
    "    history = model.fit([q1,a2],y3, batch_size=10, epochs=50, validation_data=([q4,a5],y6),callbacks = callbacks_list)\n",
    "    accuracy_history = history.history['acc']\n",
    "    val_accuracy_history = history.history['val_acc']\n",
    "    \n",
    "    print (\"Last training accuracy: \" + str(accuracy_history[-1]) + \", last validation accuracy: \" + str(val_accuracy_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c, r = Y_train.shape\n",
    "Y_train_p = Y_train.reshape(c,)\n",
    "\n",
    "for i, (train, test) in enumerate(skf.split(QA_train_padded, Y_train_p)):\n",
    "            print (\"Running Fold\", i+1, \"/\", 10)\n",
    "            train_and_evaluate__model(model, Q_train_padded[train],QA_train_padded[train],Y_train[train], Q_train_padded[test],QA_train_padded[test],Y_train[test],i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD KERAS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = load_model(\"keras_models/Q_QA_Avg.h5\", custom_objects={'OuterProduct': OuterProduct})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextProject",
   "language": "python",
   "name": "textproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
